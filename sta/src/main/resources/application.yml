server:
  port: 8848

#文件存储路径
prop:
  upload-folder: ./upload/testCases/

spring:
  servlet:
    multipart:
      max-file-size: 10MB  # 单个文件的大小
      max-request-size: 100MB  # 上传文件的总大小

  aop:
    auto: true

  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://112.74.79.124:3306/sta_oj?characterEncoding=utf8&useSSL=false&useAffectedRows=true
    username: nonameless
    password: tMU9xlqYoC6HGGE+4+NqZnujnn15M6PT+dxjoh8sDH6AyHPh1lqc2+NQ3jS+cscJxC23egNPVyVT2rQ3cBHRSA==
    #通过connectProperties属性来打开mergeSql功能；慢SQL记录
    connectionProperties: config.decrypt=true;config.decrypt.key=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALipQT36UA7znO2AXNeBspaVOjpJMKtXN7a90aOsJe6PT9P/B9ogZGq13/yzPG5RRfqP+mQCPeWxEEhKc6zuq38CAwEAAQ==;druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
    #属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有： 监控统计用的filter:stat 日志用的filter:log4j 防御sql注入的filter:wall
    filters: stat,wall,slf4j,config

  kafka:
    # kafka服务器地址，多个集群用逗号分隔
    bootstrap-servers: 112.74.79.124:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: default_consumer_group
      enable-auto-commit: true
      auto-commit-interval: 1000
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

  # mail 配置
  mail:
    username: 2835391726@qq.com
    password: xrszjaofvjycdfjh
    host: smtp.qq.com
    properties:
      mail:
        stmp:
          auth: true
          starttls:
            enable: true
            required: true

mybatis:
  configuration:
    #   下划线自动转为驼峰命名法
    map-underscore-to-camel-case: true
  type-aliases-package: com.sicnu.sta.entity
  mapper-locations: classpath:dao/*.xml

